#машинное_обучение #хэндбук

[Хэндбук](https://education.yandex.ru/handbook/ml/article/metricheskiye-metody)

**Метрические методы** фактически не обучаются, а просто запоминают обучающую выборку и на этапе предсказания ищут объекты похожие на целевой и делают на их основе предсказание. Это называется **lazy learning**. ==Подобные методы могут сойти как хороший бейзлайны.==

# Метод k-ближайших соседей (KNN)

[Наглядно о работе метода](https://yastatic.net/s3/academy/ml/knn_clf/knn_clf.html)

Быстро, для галочки, напишу, что k ближайших соседей работает так: для целевого объекта $u$ в обучающей выборке находится k ближайших объектов и отнесем к тому классу $y$, расстояние (функция $\rho$) до объектов которого минимально. Математически это можно записать следующим образом:$$a(u)=arg\max_{y\in Y}\sum_{i=1}\limits^{k}\mathbb{I}[y_{u}^{(i)}=y]$$
## Выбор метрики

![[функция_расстояния.png]]

**Евклидово расстояние:**$$\rho(x, y)=\sqrt{\sum_{i}\limits (x_{i}-y_{i})^2}$$
**Манхэттенское расстояние:**$$\rho(x,y)=\sum_{i}\limits |x_{i}-y_{i}|$$
Часто использует в высокоразмерных пространствах из-за большей устойчивости к выбросам (в отличии от евклидового мы не возводим разницу с выбросом в квадрат).

**Метрика Минковского:**$$\rho(x,y)=(\sum_{i}\limits |x_{i}-y_{i}|^p)^\frac{1}{p}$$
Есть обобщение евклидовой и манхэттенской метрик.

**Косинусное расстояние:**$$\rho(x,y)=1-\cos(\theta)=1-\frac{x\cdot y}{||x||\space||y||}$$
Как можно видеть метрика не зависит от нормы (длинны) вектора.

**Расстояния Жаккара:**$$\rho(A,B)=1-\frac{|A\cap B|}{|A \cup B|}$$
Расстояние Жаккара показывает насколько различны два множества. Интересно записать, что из единицы вычитается **коэффициент Жаккара**.

## Взвешенный KNN

Алгоритм выше никак не оценивает расстояние до объектов, то есть что далекие что очень близкие объекты вносят одинаковый вклад. Логичным шагом по модификации алгоритма будет добавление **весов** каждому расстоянию: $$a(u)=arg\max_{y\in\mathbb{Y}}\sum_{i=1}\limits^{k}w_{i}\mathbb{I}[y_{u}^{(i)}=y]$$  
Такой алгоритм и называется **взвешенный KNN**.

Веса $w_i$ могут определяться:
1. номером объекта в отсортированном по расстоянию списке обучающей выборки;
2. функцией от $\rho(x, y)$ 

В втором случае имеется **ядерная функция (kernel function)** и тогда имеем следующую модификацию:$$a(u)=arg\max_{y\in\mathbb{Y}}\sum_{i=1}\limits^{k}K(\frac{\rho(x, y)}{h})\mathbb{I}[y_{u}^{(i)}=y]$$ 

Две наиболее используемые функции:
1. $K(x)=\frac{1}{2}\mathbb{I}[|x|\leq 1]$  - прямоугольное ядро.
2. $K(x)=\frac{1}{\sqrt{2\pi}}e^{-2x^{2}}$ - гауссовское ядро (бесконечно гладкая везде).

## Kernal regression

Идея заключается в том, что, найдя k-ближайших соседей, целевому объекту определяется не их класс, а числовое значение вычисленное некоторым методом:
1. $a(u)=\frac{1}{k}\sum_{i=1}\limits^{k}y_{u}^{(i)}$ - значение целевой переменной делаем равном среднему значений ближайших;
2. $a(u)=\frac{\sum_{i=1}\limits^{k}K(\frac{\rho(u, x_{u}^{(i)})}{h})y_{u}^{(i)}}{\sum_{i=1}\limits^{k}K(\frac{\rho(u, x_{u}^{(i)})}{h})}$  (формула Надаря-Ватсона) - значение целевой переменной делаем равной взвешенному среднему значений ближайших.

Тогда, в задаче регрессии заменим функцию индекса на евклидово расстояние $y$, то есть ошибку. Выходит следующее: $$a(u)=arg\min_{y\in\mathbb{Y}}\sum_{i=1}\limits^{k}K(\rho\frac{u,x_{u}^{i}}{h})(y-y_{u}^{i})$$

# Поиск ближайших соседей

Зачем отказываться от обычного вычисления расстояния? Если у нас 1000 точек обучающей выборки и в predict подано еще 100, то, соответственно, нужно будет произвести $1000\cdot 100$ расчётов, что долго. В идеале хотим справляться за логарифмическое время.

## Точный метод

### k-d-деревья

Является **обобщением бинарного дерева поиска на данные с множеством измерений.** Каждая нода хранит в себе объект выборки, который делит собой всю выборку на два подпространства и так далее.
Рассматривая случай распределения точек в k-d-дереве имеем следующий алгоритм:
1. Корень дерева делит пространство точек на те, которые меньше его оси $x$ и те которые больше.
2. Его дочки делят свои пространства точек на те, в которых значения оси $y$ меньше или больше.

Отсюда становится ясно, что определенные уровни деревьев отвечают за свои пространственные оси. Они сменяют друг друга циклически, то есть: $x\rightarrow y\rightarrow x \rightarrow\space...$ 
За разделяющую точку чаще всего берут случайно выбранную или медианную.

Процесс нахождения соседей в этом дереве аналогичен бинарному поиску.

==Очень важно! При больших размерностях и большом количестве данных временная сложность алгоритма приближается к обыкновенному перебору, что грустно. Почему так смотри здесь [[k_d_дерево_почему_On.html]]==

## Приближенные методы

Чаще всего не обязательно находить самых-самых ближайших соседей, часто достаточно нахождения некоторого из них. 

### Random projection tree

Опять деревья, всё-таки слишком удобно разделять пространство на подпространства. 

Здесь рассмотрим алгоритм **Annoy** (он используется для рекомендации музыки в Spotify), чем-то похожий на [[#k-d-деревья|k-d-дерево]]:
1. Выбирается две случайные точки которые делят пространство на два подпространства.
2. Все, далее первый пункт повторяется пока мы не разделим изначальное пространство признаков на столько подпространств чтобы в каждом хранилось ни более$M$ объектов  ($M$ - это гипперпараметр). В результате получаем бинарное дерево.

Но одно такое разделение не гарантирует большой точности в определении ближайших соседей к целевому объекту, потому строится лес таких вот бинарных деревьев и чем больше их, тем большей точности получится добиться.

Вообще явялется компромиссом в точности и ресурсозатратности, но он плохо рабоатает с [[2.1 Линейные модели#^97208e|батчами]], а ещё помним и понимаем, что при добавлении новой точки в обучающую выборку нужно заново вычислять лес.

### Locality-sensitive hashing (LSH)

[Статейка о хэш-функциях](https://habr.com/ru/articles/509220/)
[Статейка о LSH](https://randorithms.com/2019/09/19/Visual-LSH.html)

Данный метод основан на все том же разделении точек на батчи, но уже с помощью семейства хэш-функций $H(R, cR, p_{1}, p_{2})$ таких что:
1. для $\rho(x, y) < R$ вероятность коллизии $Pr[h(x)=h(y)] > p_{1}$
2. для $\rho(x, y) > cR$ вероятность коллизии $Pr[h(x)=h(y)] < p_{2}$

Под коллизий здесь понимается вероятность попадания двух точек в один батч. 

Для каждой функции расстояния $\rho(x, y)$ существует своё семейство хэш-функций, например для евклидового и манхэттенского расстояния используются **случайные проекции**.

Аналогично методу выше можно говорить о компромиссе между временными и ресурсными затратами, но всё же имеются проблемы такие как: большие затраты по памяти (так как мы говорим о хэш-функции), а также отсутствие гарантии более быстрого поиска ближайших соседей так как кроме поиска  нужно вычислять хэши объектов.

#### Proximity graphs & Hierarchical navigable small world (HNSW)

Здесь вместо хэш-таблиц или деревьев используются графы. Все точки обучающей выборки образуют граф [[NSW_HNSW.html|NSW]], в котором, благодаря его свойствам, можно быстро добраться до ближайшего. Сам поиск осуществляется следующим образом:
1. Выбирается случайная точка в графе и среди её соседей выбирается та, которая ближе всего к целевой.
2. Далее процесс повторяется пока не будет найдена самая близкая.

В этом подходе имеется изъян - если слишком много точек, то, вполне вероятно, придется делать очень много итераций, чтобы добраться до ближайшего. 
Чтобы преодолеть это, будем разбивать граф на [[NSW_HNSW.html|слои (HNSW)]] - самый первый слой будет содержать очень мало точек изначального графа и, чем ниже слой, тем больше в нем точек. Потенциально, только на нулевом слое можно сразу же попасть в тот кластер, где находится ближайший сосед, далее будет лишь уточнение (которое можно не доводить до конца).

==На данный момент (2025) является лучшим методом организации обучающей выборки для поиска ближайшего в ней, хоть и имеет такие недостатки как большие затраты памяти на хранения слоев с вершинами и связями между ними, а также необходимость перестройки всего графа при добавлении новой точки в обучающую выборку.==




