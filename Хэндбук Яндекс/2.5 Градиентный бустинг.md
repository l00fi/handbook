#хэндбук 

[Хэндбук](https://education.yandex.ru/handbook/ml/article/gradientnyj-busting)

Этот конспект развивает идеи [[2.4 Ансамбли|предыдущего]], а именно говорит о мощнейшем на данный момент (2025) не-нейросетевых моделей - **градиентный бустинг над решающими деревьями**. 

Хорош в задачах:
- поисковом ранжировании;
- рекомендательных системах;
- таргетировании рекламы;
- предсказании погоды;
- выбора пункта назначения такси и многих других.

Однако с однородными данными, такими как видео, изображения, звук справляется хуже нейронных сетей. 

**Основная идея** градиентного бустинга заключается в поэтапной компенсации ошибки предыдущих моделей, то есть:$$L(y, a_{k+1}(x))<L(y, a_k(x))$$
На примере, имея в $k$-ой модели ошибку 10, следующую модель $k+1$ мы строим таким образом, чтобы она предсказывала эту ошибку с другим знаком. Следовательно, сложив эти две модели, получаем в точности нужный ответ.

==Бустинг можно использовать и для других семейств моделей, но не понятно имеет ли это много смысла, ведь даже не точная модель будет в конечном итоге достаточно точна, а хлопот с её реализацией будет меньше.==

При первом шаге бустинга модель учится на признаках $X$ и таргете $y$, а последующие модели учатся на признаках $X$ и ошибке предыдущей модели: $s^{k-1}=y_{i}-a_{k-1}(x_{i})$.

# Математическое обоснование

Пусть $L$ - дифференцируемая функция, $a(x)=b_{1}(x)+b_{2}(x)+...b_{k}(x)$ - композиция базовых алгоритмом, причем:$$a_{k}(x)=a_{k-1}(x)+b_{k}(x)$$$$b_{k}=arg\min_{b\in\mathbb{B}}\sum_{i=1}\limits^{N} L(y_{i}, a_{k-1}(x_{i}) + b(x_{i}))$$ $$b_{0}=arg\min_{b\in\mathbb{B}}\sum_{i=1}\limits^{N} L(y_{i}, b(x_{i}))$$
Если разложить функцию $L$ в [ряд Тейлора](https://ru.wikipedia.org/wiki/%D0%A0%D1%8F%D0%B4_%D0%A2%D0%B5%D0%B9%D0%BB%D0%BE%D1%80%D0%B0) до первого члена в точке $(y_{i}, a_{k-1}(x_{i}))$: $$L(y_{i}, a_{k-1}(x_{i})+b(x_{i}))\approx L(y_{i}, a_{k-1}(x_{i}))+b(x_{i})\frac{\delta L(y_{i},z)}{\delta z}_{z=a_{k-1}(x_{i})}=L(y_{i}, a_{k-1}(x_{i}))+b(x_{i})g_{i}^{k-1}$$
Убирая постоянные члены получим:$$b_{k}\approx arg\min_{b\in\mathbb{B}}\sum_{i=1}\limits^{N}b(x_{i})g_{i}^{k-1}$$

Конспект небольшой, так как переписывать всю статью не хочется и лить воду тоже. Если есть вопросы - читай хэндбук.





