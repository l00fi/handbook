#хэндбук #машинное_обучение 

[Хэндбук](https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii)

# Энтропия и дивергенция Кульбака-Лейблера

Измерить "знание" можно с помощью **энтропии Шэнyона**:$$H(P)=-\sum_{x}\limits{P(x)logP(x)}$$ 
Энтропия показывает, сколько в среднем бит нужно для передачи информации о значении случайно величины.

**Дивергенция Кульбака-Лейблера**: $$KL(p||q) = \int{p(x)log \frac{p(x)}{g(x)}dx}$$

Данное выражение показывает расстояние между распределениями $p$ и $q$. Также оно не симметрично, то есть: $KL(p||q) \neq KL(q||p)$

# Принцип максимальной энтропии

Среди всех распределений заданных на $\mathbb{X}$ и удовлетворяющих условию $\mathbb{E}_{u_{1}(x)}= \mu_{1},...,\mathbb{E}_{u_{n}(x)}= \mu_{n}$, где $u_{i}$ - некоторая функция, наиболее предпочтительны те что имеют наибольшую энтропию. Это так, потому что нужна наиболее "случайная" величина.