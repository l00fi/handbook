#машинное_обучение #хэндбук

[Хэндбук](https://education.yandex.ru/handbook/ml/article/linear-models)
# Линейные модели

Линейная функция: $y = w_{1}x_{1}+ ... + w_{D}x_{D} + w_{0}$ ,где 
1. $y$ – целевая переменная (**таргет**), $(x_{1},…,x_{D})$ – вектор, соответствующий объекту выборки (**вектор признаков**)
2. $w_{1},…,w_{D}​,w_{0}$​ – параметры модели. Признаки ещё называют **фичами** (от английского **features**)
3. $w=(w_{1},…,w_{D})$ часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта
4. $w_{0}$​ – свободным коэффициентом, или **сдвигом** (**bias**).
Получается, что мы ищем вектор $(w_0​,w_1​,…,w_{D}​) \in \mathbb{R^{D+1}}$.

Если необходимо работать с категориальными данными для модели линейной регрессии одним из самых простых способов является **one-hot кодирование** (**one-hot encoding**). Суть в том, что исходный категориальный признак может принимать $M$ значений $c_1,…,c_M$​. Заменим категориальный признак на $M$ признаков, которые принимают значения $0$ и $1$: $i$-й будет отвечать на вопрос «принимает ли признак значение $c_i$​?».

Наличие «лишних» признаков ведёт к переобучению или вовсе ломает модель.

Если вес $w_i$ положителен, то с ростом $i$-го признака таргет в случае регрессии будет увеличиваться и наоборот, с классификацией мы будем больше склняться к тому или иному классу. А чем больше значение веса $w_i$ тем важнее $i$-й признак для итогового предсказания. Данное качество модели называется **интерпретируемость**. 

Но интерпретировать значения весов итоговой модели будет проблематично в силу следующего:
1. Для более сложных задач моделей линейной регрессии может потребоваться введение дополнительных признаков которые есть сложные функции от изначальных признаков. Поэтому интерпретировать значения весов итоговой модели будет проблематично.
2. Если между признаками присутствует какая-то зависимость.
3. Значения весов могут меняться от обучающей выборки! Но, при увеличении размера обучающей выборки весы будут стремиться к "идеальному" значению.

# Линейная регрессия и МНК

Напишу тривиальное, для галочки, - для решений задач регрессии используем линейные модели 

Результаты градиентного бустинга и нейронных сетей интерпретировать довольно сложно.

Свободный член $w_0$​ часто опускают, потому что такого же результата можно добиться, добавив признак $x_0$, равный единице; тогда роль свободного члена будет играть соответствующий ему вес.

Функция, оценивающая то, как часто модель ошибается, традиционно называется **функцией потерь**, **функционалом качества** или просто **лоссом** (**loss function**). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно.

$L^2$-норма разницы – это евклидово расстояние $(∥y−f_w(x)∥_2)^2$ = $(\sqrt{\sum_{i=1}^{N}\limits{(y_i-f_w(x_i))^2}})^2​$ между вектором таргетов и вектором ответов модели.

## MSE (среднее квадратичное отклонение)
$$\frac{1}{N}\sum_{i=1}^{N}\limits{(y_i-f_w(x_i))^2}$$
То есть $L^{2}$-норма разницы деленная на количество объектов в выборке, так как обычная норма не слишком информативна при сравнении модели на разных объемах данных (на данных с бОльшим объемом ошибка будет расти вместо с ростом количества объектов). То есть MSE можно записать следующим образом:
$$MSE(f,X,y)=\frac{1}{N}∥y−Xw∥^2$$
если в матричном виде, то получаем следующее: $$MSE(f,X,y)=\frac{1}{N}X^{T}(Xw-y)^2$$
==Фан-факт - отображение, которое на вход принимает функцию, а на выходе выдаёт число называют **функционалом**.==
Можно заметить, что MSE и есть функционал: она сопоставляет каждой функции $f$ с определенными весами $w$ некоторое число. Основная задача состоит в том, чтобы уменьшить это самое число насколько это возможно, потому что в основу MSE лежит $L^{2}$-норма, показывающая расстояние от действительной точки до предсказанное точки.

==Смотри [[объяснение_разложения_y.html]] чтобы понять почему $y=y_{||}+y_{\perp}$==

Если говорить о точной формуле весов $w$, то она следующая: $$w=(X^{T}X)^{-1}X^{T}y$$ 
$(X^{T}X)^{-1}X^{T}$ - псевдообратная матрица Мура-Пенроуза

Проблема метода состоит в следующем:
1. Обращать огромные матрицы затратно по вычислительным ресурсам;
2. Может оказаться так, что некоторые $x$ могут быть приближенно линейно зависимы, из этого следует, что веса $w$ будут зависеть от квадрата (смотри формулу выше) числа обусловленности матрицы $X$. То есть малое изменение таргета $y$ ведет к огромным изменениям весов $w$. 

==Фан-факт - **число обусловленности** определение из области численного анализа описывающая изменение функции при небольшом изменении её аргумента, то есть:==
$y=f(x)$
${y+\Delta{y}}=f(x+\Delta{x})$
$\mu(f)=\max_{x}\frac{\varepsilon_y}{\varepsilon_{x}}=\max_x\frac{||\Delta{y}||/||y||}{||\Delta{x}||/||x||}$
Это написано к дополнению о минусах нахождения точного значения $w$. При наличии приближенной линейной зависимости признаков вес $w$ зависит как раз от числа обусловленности матрицы $X$, что ведет к огромным изменениям веса при небольшом изменении таргета, то есть $y$.

==Фан-факт - [видосик про сингулярные векторы, писать слишком много](https://youtu.be/mfn_2d_lLxM?si=Qp9L-BiKHg-QXxra)==

А теперь, так как функционал гладкий, непрерывный и выпуклый, в нем можно найти минимумы с помощью **итеративных градиентных методов**.

## Градиентный спуск

Градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания.

$$w_{i}\rightarrow w_{i} - \alpha\frac{d}{dw_i}L(f_{w,}X,y)$$
1. $L(f_{w,}X,y)$ - функция потерь с функцией регрессии $f_w$ 
2. $\alpha$ - коэффициент обучения

Соответственно нужен сам градиент: $$\nabla_{w}L = \frac{2}{N}X^{T}(Xw-y)$$
Алгоритм на питончике:
```
w = random_normal() # можно пробовать и другие виды инициализации 
repeat S times: # другой вариант: while abs(err) > tolerance 
	f = X.dot(w) # посчитать предсказание 
	err = f - y # посчитать ошибку 
	grad = 2 * X.T.dot(err) / N # посчитать градиент 
	w -= alpha * grad # обновить веса
```

## Стохастический градиентный спуск

Один из методов оптимизации градиентного спуска это **стохастический градиентный спуск**

Можно заметить, что полный градиент можно воспринимать как среднее градиентов по всем объектам, то есть математическое ожидание полного градиента. Тогда среднее градиентов объектов подвыборки также можно понимать как математическое ожидание полного градиента подвыборки. 
Тогда можно заменить градиент его оценкой на подвыборке, которая называется **batch**.
$$\nabla_{w}L(w, X, y) = \frac{1}{N}\sum_{i=1}\limits^{N}\nabla_{w}L(w, x_{i,}y_i)$$
$$\nabla_{w}L(w, X, y)\approx\frac{1}{B}\sum_{i=1}\limits^{b}\nabla_{w}L(w, x_{i_t}y_{i_t})$$
Стохастический градиентный спуск будет работать следующим образом:  ^97208e
1. Случайно перемешав всю выборку выбирается $B$ первых элементов на которых вычисляется градиент и обновляются веса. Так проходимся по всей выборке.
2. Затем повторяем вышеописанное $E$ (эпоха) раз, то есть $E$ равен количество полных проходов по выборке.  

Алгоритм на питончике:
```
w = normal(0, 1) 
repeat E times: 
	for i = B, i <= n, i += B 
		X_batch = X[i-B : i] 
		y_batch = y[i-B : i] 
		f = X_batch.dot(w) # посчитать предсказание 
		err = f - y_batch # посчитать ошибку 
		grad = 2 * X_batch.T.dot(err) / B # посчитать градиент 
		w -= alpha * grad
```

Но для сложных моделей и сложных функций ошибки (**лосов**) стохастический градиентный спуск может сходится медленно или застревать в локальных минимумах.

Существуют и не градиентный методы, но они работают гораздо хуже и не имеют такой математической базы. 

# Регуляризация

Ещё одна статейка [тут](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F)

Может случится, что веса регрессии $w$ неразумно велики или имеют неадекватный знак при каком-то признаке $x_{i}$. Это может свидетельствовать о нескольких вещах: 
1. приближенной линейной зависимости между некоторыми парами векторов признаков, что называется **мультиколлинеарностью**;
2. о переобучении модели (хотя, возможно, это есть следствие первого).

С этим и призвана бороться **регуляризация**, которая добавляет дополнительные ограничения на вектор весов. Ограничения задаются по разному, ниже опишу $L^1$ и $L^{2}$ нормы.

В общем случае смысл задачи регуляризации сводится к следующему: $$\min_{w}L(f, X, y) = \min_{w}(||Xw - y||_{2}^{2} + \lambda||w||_{k}^{k})$$
В случае $L^{1}$ имеем: $$L(f_{w},X,y)=||Xw-y||_{2}^{2}+\lambda{|w|_{1}}$$В случае $L^{2}$ имеем: $$L(f_{w},X,y)=||Xw-y||_{2}^{2}+\lambda{|w|_{2}^{2}}$$
$$||w||_2^{2}=w_{1}^{2}+...w_{D}^{2}$$
$\lambda||w||^{k}$ - называется **регулятором** или **регуляризационным членом**, а $\lambda$ - коэффициентом регуляризации (гиперпараметр модели который подбирают по логарифмической шкале).

Соответственно **точное решение** при $L^{2}$-регуляризации будет выглядеть так:$$w=(X^{T}X+\lambda{I})^{-1}X^{T}y$$
Очевидно, собственно, чтобы избежать вырожденности $X^{T}X$ будем просто добавлять к нему какой-то коэффициент $\lambda$. Выходит, что решение окажется менее точным, но его решение будет более качественным.

А **градиент** будет выглядеть: $$\nabla_{w}L(f_{w}, X, y)=2X^{T}(Xw-y) + 2\lambda{w}$$
==Стоит отметить, что $L^{2}$ используется в большинстве задач, но $L^{1}$ имеет полезную особенность, что признаки с малыми весами после оптимизации обращаются в 0, а также для признаков с приближенной линейной связью.== 

## Другие лоссы

Дальше разговор идет о некоторых функциях ошибки:
1. **MAE** (mean absolute error) - это сумма модулей разности фактического таргета и предсказанного деленная на размер выборки. В отличии от MSE вклад больших ошибок уменьшается. Уместна в тех случаях, когда в таргете данных много выбросов.
2. **MAPE** (mean absolute percentage error) - $$\frac{1}{N} \sum_{i=1}\limits^{N}\frac{|\bar{y_i} - y_i|}{y_i}$$Часто используется в задачах прогнозирования, когда мы пытаемся угадать порядок величины, то есть нет нужды больше штрафовать за предсказание 2000 вместо 1000, чем за 2 вместо 1.

# Линейная классификация

Задача состоит в том, чтобы разделить объекты на классы $\{0;1\}$ по некоторым признакам. То есть итоговый функция классификации есть ни что иное как: $$y=sign\langle x_{i};w \rangle$$
==Если данные выборки можно разделить прямой, то данная выборка называется линейно разделимой. ==
Желание приспособить линию регрессии для это задачи сразу стоит отбросить, так как задача состоит не в проведении линии через наибольшее количество точек, а такой линии которая **наилучшим образом разделит их**. 

Введем понятие **разделяющего правила**:$$\mathbb{I}[x\geq x_0]$$Которое определяет условие при котором мы отнесем $x$ к тому или иному классу. Представляет собой прямую или гиперплоскость, которая оптимально разделяет объекты на два (или более классов).

Теперь нужно сформулировать задачу более строго - нужно минимизировать ситуации, когда модель неправильно предсказывает метку класса для объекта, то есть записать это можно следующим образом: $$\sum_{i=1}\limits\mathbb{I}[y_{i}
\langle x_{i};w \rangle < 0]\rightarrow\min_{w}$$
==Смотри [[отступ_классификатора_скалярное_предсказание_классификатора.html]]==

$\langle x_{i};w_{i} \rangle$ есть оценка насколько близко или далеко к разделяющей плоскости находится объект, соответственно, чем больше значение, тем более уверенно модель говорит о принадлежности тому или иному классу. Домножаем на метку класса $y_i$ что понять правильно сделано предсказание или нет. Если, например, оба множителя отрицательны или положительны, то они больше нуля и предсказание верно, иначе объект определен в не правильный класс.

$M = y_{i}\langle x_{i};w \rangle$ называется **отступом классификатора**, а описанная выше функция потерь называется **missclassification loss**. 
 
 Для каждого класса считается функция: $$F(M)=\mathbb{I}[M < 0] = \begin{cases} 1, M < 0 \\ 0, M \geq 0 \end{cases}$$
То есть, возвращаясь к сумме выше, в идеальном случае, хочется получить сумму значений функции для отступа каждого из объектов равной нулю.

==Подробно не записываю, но стоит упоминания перцептрон Розенблата - первая вычислительная модель нейросети, которая привел к концепции глубокого обучения. Решает задачу линейной классификации и имеет неприятную особенность - предлагаемые ею правила не единственны и сильно зависят от начального состояния, то есть на одном и том же датасете с двумя разными обучающими выборками будут разные разделяющие правила.==

В идеале нужно найти не только такую плоскость, которая будет правильно разделять объекты на классы, но и чтобы отступ от этой самой плоскости для каждого из объектов был максимален. 

## Метод опорных векторов (SVM)

Как раз для выполнения описанной выше задачи придуман **метод опорных векторов**, математически описывается так: 
$F(M)=max(0,1-M)$
$L(w, x, y)=\lambda||w||_{2}^{2}+\sum_{i}\limits F(M)$  - а здесь у нас $L^2$-регуляризация.
$\nabla L(w, x, y)=2\lambda w + \sum_{i}\limits \begin{cases} 0, 1 - M \leq 0 \\ -y_{i}x_{i}, 1 - M > 0  \end{cases}$  
Единица здесь существует для того, чтобы правильные, но не самые уверенные предсказания тоже вносили свой вклад в итоговое разделяющее правило.

Если посмотреть на график ниже
![[классификация_SVM.png]]
То видно, что положение классификатора или разделяющего правила определяется не всеми объектами, а самыми близкими к нему, которые называются **опорными векторами**. Отсюда и название всего метода.

==Фан-факт - с 60-х годов это был самый сильный метод машинного обучения, в силу своей ресурсоемкости, затем его сменили деревья решений, а ныне (2025 г.) актуальны нейросети.==

## Логистическая регрессия

Можно посмотреть на задачу классификации как на вероятностную оценку принадлежности объекта к тому или иному классу. Если вероятность будет $>0.5$ то объект принадлежит к классу $1$, иначе $0$. в
Возникает проблема - вероятность может принимать значения от 0 до 1. Как обучить линейную модель таким образом, чтобы на выходе соблюдалось это условие? Используется логарифм шансов (смотри [[логарифм_шансов_и_предпосылка_к_выводу_сигмоиды.html]]): $$log(\frac{p}{{1-p}})=\begin{cases}-c, p < 0.5 \\ c, p > 0.5 \end{cases}, c - const$$
Отсюда можно вывести функция которая называется **сигмоида**: 
$\langle w, x_{i} \rangle=log(\frac{p}{1-p})$
$e^{\langle w,x_{i} \rangle}=\frac{p}{1-p}$ 
$p=\frac{1}{1 + e^{\langle w,x_{i} \rangle}}$
Обозначается так: $\sigma(z)=\frac{1}{1 + e^{-z}}$ 

Получается, что предсказания модели вычисляются следующим образом: $$p=\sigma(\langle w,x_{i} \rangle)$$
Функция потерь имеет вид (вывод большой, смотри хэндбук):$$L(w, X, y)=-\sum_{i}\limits(y_{i}log(\sigma(\langle w,x_{i} \rangle))+(1 - y_{i})log(\sigma(-\langle w,x_{i} \rangle)))$$
Явного точного решения для этого монстра нет, поэтому описываем **градиент**:$$\nabla L(w, X, y)=-\sum_{i}\limits x_{i}(y_{i}-\sigma(\langle w, x_{i} \rangle))$$
___
