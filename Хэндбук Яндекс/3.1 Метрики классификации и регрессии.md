#хэндбук #машинное_обучение 

[Хэндбук](https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii)

Метрики делятся на:
1. **Online** - качество модели измеряется уже в процессе её использования на реальных данных;
2. **Offline** - качество модели измеряется до выпуска модели в продакшн.

Функция потерь **не есть** метрика качества. **Первую мы оптимизируем**, чтобы получить хорошую модель, то есть настроить внутренние веса и т.д, а **вторая - это внешняя объективная оценка** не знающая о внутреннем устройстве модели, а лишь оценивающая её предсказания.

# Рассмотрение основных метрика качества на примере бинарной классификации
## Accuracy
**Accuracy** - доля объектов для которых верно предсказан класс:$$accuracy(y, y^{pred})=\frac{1}{N}\sum_{i=1}\limits^{N}\mathbb{I}[y_{i}=f(x_{i})]$$$$error\space rate=1-accuracy$$
Минусы:
1. Не учитывает дисбаланс классов;
2. Не наказывает за ошибки.
## Confusion matrix
При предсказании объекта может сложится 4 ситуации:
- **TP** -  положительный объект (тот, чьё правильное предсказание нам важнее остальных) предсказан верно;
- **FP** - положительный объект предсказан неверно;
- **TN** - отрицательный объект (класс которого нам не важен) предсказан верно;
- **FN** - отрицательный объект предсказан неверно.
==Чтобы запомнить - первая буква говорит о том, успешно или нет предсказание, вторая говорит о том что мы предсказываем.==
Эти параметры чаще всего представляют в виде матрицы:$$\begin{pmatrix}TP & FN \\ FP & TN\end{pmatrix}$$ В идеале сумма значений главной диагонали должна быть равна количество объектов переданных модели для предсказания. Легко заметить, что **доля объектов, попавших на главную диагональ - это accuracy.**
Бывают ситуации, когда нам нужно взвесить какие-то из этих 4 показателей. Какой-то из них может быть важнее, поэтом вводятся следующие метрики.
## Точность (Precision) и полнота (Recall)
**Precision** - отражает долю правильно предсказанных объектов положительного класса среди всех объектов которым предсказан положительный класс.
$$precision = \frac{TP}{TP + FP}$$
**Recall** - отражает долю объектов верно признанных как представители положительного класса ко всем объектам действительно являющимися представителями положительного класса.
$$recall = \frac{TP}{TP + FN}$$
Приятная поясняющая выдержка из хэндбука:
```
Например, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно злокачественные, а полнота — какую долю злокачественных опухолей нам удалось выявить.
```
## F1-мера
F1-мера есть численное выражение Precision и Recall метрик одновременно и равно среднему гармоническому от них:$$F_{1}=\frac{2}{\frac{1}{recall} + \frac{1}{Precision}}$$ При условии, что обе метрики нам одинаково важны, если какая-то из метрик имеет иной вес чем другая, то имеет место: $$F_{\beta}=(\beta^{2} + 1)\frac{recall\cdot precision}{recall + \beta^{2}precision}$$
==Почему не можем взять среднее геометрическое, арифметическое или квадратное?==
```
Потому что иные средние не делают акцента на низких значениях, если посмотреть на формулу среднего гармонического, то видно, что при низком значении одной из метрик F1-мера тоже будет мала - такое не происходит в среднем квадратичном или арифметичвеском, а геометрическое не достаточно низко занижает меру.
```
## Бинарная классификация на вероятностных моделях. AUC метрика.
**TPR** - доля положительных объектов, правильно предсказанные положительными.
$$TRP = \frac{TP}{P}=\frac{TP}{TP+FN}$$
**FPR** - доля отрицательных объектов, неправильно предсказанных положительными:
$$FRP=\frac{FP}{N}=\frac{FP}{FP+TN}$$
Если уменьшить порог после которого вероятностная модель признаёт объект положительным, то обе эти меры **растут**.
Если построить [график](https://yastatic.net/s3/academy/ml/roc_auc/roc.html) с осями TPR/FPR, то кривая образуемая варьировании порога называется **ROC-curve** (Самая универсальная метрика классификации).
==Чем лучше классификатор разделяет объекты на классы, тем больше площадь под кривой - это и есть **AUC-метрика**.==

[Хорошая статейка по этому поводу](https://alexanderdyakonov.wordpress.com/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/)

Имеет смысл применять тогда, когда важны не сами классы, а их правильный порядок при предсказании.

## Average precision
Построим график, где по оси Y - значения Precision, а по оси X - значения Recall. Постоянно изменяя значения порога (то есть уверенность модели в том что объект относится к положительному классу) и пересчитывая метрики, получим кривую. Площадь под этой кривой и есть **average precision**, то есть:$$AP=\int_{0}^{1}p(r)dr$$ И эта площадь отражает **среднее значение точности**. Очевидно, что нужно это значение максимизировать. 
==Зачем она и где применяется?==
```
Мы рассматрвиаем precision на разных уровнях recall, варьируя порог, то есть понижая или увеличивая увернность модели в предсказании положительного класса. Отсюда следует, что метрика помогает найти оптимальный порог.
Применяется в случах, когда имеется дисбаланс классов, при ранжировании предсказаний или в компьютерном зрении.
```

**Работает только для метрических моделей!!!**

## Топ k классов

Обобщение метрики **accuracy** для k объектов. 
$$tha(y, \hat{f})=\frac{1}{n}\sum\limits_{i=0}^{n-1}\sum\limits_{j=1}^{k}\mathbb{I}(\hat{f}_{ij}=y_i)$$

# Метрики

**MAPE**$$MAPE(y^{true}, y^{pred})=\frac{1}{N}\sum_{j=1}\limits^{N}\frac{|y_{j}-f(x_{j})|}{|y_{j}|}$$

**SMAPE**$$SMAPE(y^{true}, y^{pred})=\frac{1}{N}\sum_{j=1}\limits^{N}\frac{2|y_{j}-f(x_{j})|}{y_{j}+f(x_{j})}$$

**WAPE**$$WAPE(y^{true}, y^{pred})=\frac{\sum_{j=1}\limits^{N}|y_{j}-f(x_{j})|}{\sum_{j=1}\limits^{N}|y_{j}|}$$

**RMLSE**$$RMLSE(y^{true}, y^{pred}|c)=\sqrt{\frac{1}{N}\sum_{j=1}\limits^{N}(log(y_{j}+c)-log(f(x_{j}+c))^{2}}$$
